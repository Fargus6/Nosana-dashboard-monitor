#!/usr/bin/env python3
"""
Accurate day-by-day breakdown with proper 24-hour periods
"""
import asyncio
import sys
sys.path.append('/app/backend')

from datetime import datetime, timezone, timedelta
from collections import defaultdict
import os
os.environ['PLAYWRIGHT_BROWSERS_PATH'] = '/pw-browsers'

from playwright.async_api import async_playwright
import re

async def parse_relative_time(time_text: str, scrape_time: datetime) -> datetime:
    """Convert relative time to datetime"""
    try:
        # Days
        days_match = re.search(r'(\d+)\s+days?\s+ago', time_text)
        if days_match:
            return scrape_time - timedelta(days=int(days_match.group(1)))
        
        # Hours
        hours_match = re.search(r'(\d+)\s+hours?\s+ago', time_text)
        if hours_match:
            return scrape_time - timedelta(hours=int(hours_match.group(1)))
        
        # Minutes
        minutes_match = re.search(r'(\d+)\s+minutes?\s+ago', time_text)
        if minutes_match:
            return scrape_time - timedelta(minutes=int(minutes_match.group(1)))
        
        return scrape_time
    except Exception as e:
        return scrape_time

async def accurate_day_breakdown():
    node_address = "9hsWPkJUBiDQnc2p7dKi2gMKHp6LwscA6Z5qAF8NGsyV"
    
    scrape_time = datetime.now(timezone.utc)
    print(f"\n{'='*100}")
    print(f"ACCURATE DAY-BY-DAY BREAKDOWN - NODE: {node_address}")
    print(f"{'='*100}")
    print(f"ðŸ• Scrape Time: {scrape_time.strftime('%Y-%m-%d %H:%M:%S %Z')}\n")
    
    async with async_playwright() as p:
        browser = await p.chromium.launch(headless=True)
        page = await browser.new_page()
        
        url = f"https://dashboard.nosana.com/host/{node_address}"
        print(f"ðŸ“¡ Fetching from: {url}\n")
        
        await page.goto(url, wait_until='networkidle', timeout=30000)
        await page.wait_for_selector('table', timeout=10000)
        
        # Get total number of pages
        total_pages = await page.evaluate('''() => {
            const nav = document.querySelector('[role="navigation"]');
            if (!nav) return 1;
            
            const links = nav.querySelectorAll('a.pagination-link');
            let maxPage = 1;
            
            links.forEach(link => {
                const text = link.textContent.trim();
                const pageNum = parseInt(text);
                if (!isNaN(pageNum) && pageNum > maxPage) {
                    maxPage = pageNum;
                }
            });
            
            return maxPage;
        }''')
        
        print(f"ðŸ“Š Found {total_pages} pages total")
        print(f"ðŸ”„ Scraping all pages...\n")
        
        all_jobs_data = []
        
        for page_num in range(1, total_pages + 1):
            print(f"   Page {page_num}/{total_pages}...", end='\r')
            
            if page_num > 1:
                try:
                    page_link = await page.query_selector(f'a.pagination-link:has-text("{page_num}")')
                    if page_link:
                        await page_link.click()
                        await page.wait_for_timeout(1500)
                except Exception as e:
                    continue
            
            jobs_on_page = await page.evaluate('''() => {
                const jobs = [];
                const table = document.querySelector('table');
                if (!table) return jobs;
                
                const rows = table.querySelectorAll('tr');
                
                for (let i = 1; i < rows.length; i++) {
                    const row = rows[i];
                    const cells = row.querySelectorAll('td');
                    
                    if (cells.length >= 6) {
                        const jobLink = cells[0].querySelector('a');
                        const jobId = jobLink ? jobLink.getAttribute('href').split('/').pop() : null;
                        
                        const priceText = cells[4].textContent.trim();
                        
                        jobs.push({
                            job_id: jobId,
                            started: cells[2].textContent.trim(),
                            duration: cells[3].textContent.trim(),
                            price: priceText,
                            gpu: cells[5].textContent.trim(),
                            status: cells[6].textContent.trim().includes('RUNNING') ? 'RUNNING' : 'SUCCESS'
                        });
                    }
                }
                
                return jobs;
            }''')
            
            all_jobs_data.extend(jobs_on_page)
        
        await browser.close()
        
        print(f"\nâœ… Scraped {len(all_jobs_data)} total jobs from {total_pages} pages\n")
        print("="*100 + "\n")
        
        # Process jobs and group by calendar day
        jobs_by_day = defaultdict(list)
        
        for job in all_jobs_data:
            if job['status'] != 'SUCCESS':
                continue
            
            price_text = job['price'].replace('/h', '').replace('$', '').strip()
            
            try:
                actual_payment = float(price_text)
            except:
                continue
            
            # Parse duration
            duration_seconds = 0
            duration_text = job['duration']
            
            hours_match = re.search(r'(\d+)h', duration_text)
            if hours_match:
                duration_seconds += int(hours_match.group(1)) * 3600
            
            minutes_match = re.search(r'(\d+)m', duration_text)
            if minutes_match:
                duration_seconds += int(minutes_match.group(1)) * 60
            
            seconds_match = re.search(r'(\d+)s', duration_text)
            if seconds_match:
                duration_seconds += int(seconds_match.group(1))
            
            # Calculate actual completion time
            started_time = await parse_relative_time(job['started'], scrape_time)
            completed_time = started_time + timedelta(seconds=duration_seconds)
            
            # Get the calendar day (date only, no time)
            completed_date = completed_time.date()
            
            jobs_by_day[completed_date].append({
                'payment': actual_payment,
                'completed_time': completed_time,
                'job': job
            })
        
        # Sort days in reverse chronological order (most recent first)
        sorted_days = sorted(jobs_by_day.keys(), reverse=True)
        
        print("="*100)
        print("ðŸ“Š DAY-BY-DAY EARNINGS BREAKDOWN")
        print("="*100 + "\n")
        
        total_all_days = 0
        
        for day in sorted_days:
            jobs_on_day = jobs_by_day[day]
            day_total = sum(j['payment'] for j in jobs_on_day)
            total_all_days += day_total
            
            # Format day name
            day_name = day.strftime('%A, %B %d, %Y')
            
            print(f"ðŸ“… {day_name}")
            print(f"   Jobs completed: {len(jobs_on_day)}")
            
            # Show first 10 jobs for the day
            if len(jobs_on_day) <= 10:
                for job_info in sorted(jobs_on_day, key=lambda x: x['completed_time']):
                    time_str = job_info['completed_time'].strftime('%H:%M:%S')
                    print(f"   â€¢ {time_str}: ${job_info['payment']:.3f}")
            else:
                for job_info in sorted(jobs_on_day, key=lambda x: x['completed_time'])[:10]:
                    time_str = job_info['completed_time'].strftime('%H:%M:%S')
                    print(f"   â€¢ {time_str}: ${job_info['payment']:.3f}")
                print(f"   ... and {len(jobs_on_day) - 10} more jobs")
            
            print(f"   âž¡ï¸  TOTAL: ${day_total:.3f}\n")
        
        # Calculate specific periods
        today_start = scrape_time.replace(hour=0, minute=0, second=0, microsecond=0)
        yesterday_start = today_start - timedelta(days=1)
        month_start = scrape_time.replace(day=1, hour=0, minute=0, second=0, microsecond=0)
        
        today_total = sum(
            j['payment'] 
            for day_jobs in jobs_by_day.values() 
            for j in day_jobs 
            if j['completed_time'] >= today_start
        )
        
        yesterday_total = sum(
            j['payment'] 
            for day_jobs in jobs_by_day.values() 
            for j in day_jobs 
            if yesterday_start <= j['completed_time'] < today_start
        )
        
        month_total = sum(
            j['payment'] 
            for day_jobs in jobs_by_day.values() 
            for j in day_jobs 
            if j['completed_time'] >= month_start
        )
        
        print("="*100)
        print("ðŸ“Š SUMMARY")
        print("="*100 + "\n")
        
        print(f"ðŸ“… TODAY ({today_start.strftime('%Y-%m-%d')}):")
        print(f"   Total: ${today_total:.3f}\n")
        
        print(f"ðŸ“… YESTERDAY ({yesterday_start.strftime('%Y-%m-%d')}):")
        print(f"   Total: ${yesterday_total:.3f}\n")
        
        print(f"ðŸ“… THIS MONTH ({month_start.strftime('%B %Y')}):")
        print(f"   Total: ${month_total:.3f}\n")
        
        print(f"ðŸ“… ALL TIME (from visible history):")
        print(f"   Total: ${total_all_days:.3f}\n")
        
        print("="*100)
        print("âœ… Each job payment ($0.176) scraped directly from dashboard")
        print("âœ… Jobs correctly grouped by 24-hour calendar days (midnight to midnight UTC)")
        print("="*100 + "\n")

if __name__ == "__main__":
    asyncio.run(accurate_day_breakdown())
